{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debdeep23/transformer-interpretability/blob/main/Transformer_Interpretability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "4eSEBINPmanf",
        "outputId": "b3a251ac-14c2-4873-cc34-4dba3bb981f8"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Pico-LLM: Transformer Interpretability Analysis\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Base dependencies\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import gc  # For garbage collection\n",
        "import re  # For text normalization\n",
        "from tqdm.auto import tqdm  # Notebook-friendly progress bars\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML  # For rich display in notebooks\n",
        "from collections import defaultdict\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import networkx as nx\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import sys\n",
        "\n",
        "# For data loading and tokenization\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    import tiktoken\n",
        "except ImportError:\n",
        "    print(\"Please install required packages: pip install datasets tiktoken pandas seaborn networkx matplotlib scipy scikit-learn\")\n",
        "    raise\n",
        "\n",
        "# Detect if running in Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# Setup downloads for Colab\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    print(\"Running in Google Colab - Auto-download functionality enabled!\")\n",
        "\n",
        "    # Try to mount Google Drive for additional backup\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        DRIVE_AVAILABLE = True\n",
        "        print(\"Google Drive mounted successfully for backup!\")\n",
        "    except:\n",
        "        DRIVE_AVAILABLE = False\n",
        "        print(\"Could not mount Google Drive. Will still download files to your computer.\")\n",
        "else:\n",
        "    print(\"Not running in Colab - download functionality not needed.\")\n",
        "\n",
        "# --- Global instances (will be initialized in main) ---\n",
        "tracker = None\n",
        "global_encoder = None\n",
        "\n",
        "# --- Results Directory ---\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "#################################################################################\n",
        "# Download Helper Functions for Colab\n",
        "#################################################################################\n",
        "\n",
        "def auto_download_results(result_dir, prefix=\"pico_llm\"):\n",
        "    \"\"\"\n",
        "    Compress and download results directory to local machine\n",
        "    For use in Google Colab\n",
        "    \"\"\"\n",
        "    if not IN_COLAB:\n",
        "        print(\"Not running in Colab - download function not needed\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        zip_filename = f\"{prefix}_{timestamp}.zip\"\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"COMPRESSING RESULTS FOR DOWNLOAD: {zip_filename}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Compress results\n",
        "        !zip -r {zip_filename} {result_dir}\n",
        "\n",
        "        # Download to local machine\n",
        "        files.download(zip_filename)\n",
        "\n",
        "        print(f\"\\nDownload initiated for {zip_filename}\")\n",
        "        print(\"If the download dialog doesn't appear, check your browser's download settings\")\n",
        "        print(\"or look for the file in your downloads folder.\")\n",
        "\n",
        "        # Backup to Google Drive if available\n",
        "        if DRIVE_AVAILABLE:\n",
        "            drive_backup_dir = \"/content/drive/MyDrive/pico_llm_backups\"\n",
        "            os.makedirs(drive_backup_dir, exist_ok=True)\n",
        "            drive_path = f\"{drive_backup_dir}/{zip_filename}\"\n",
        "            !cp {zip_filename} {drive_path}\n",
        "            print(f\"\\nBackup also saved to Google Drive: {drive_path}\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error during download: {e}\")\n",
        "        print(\"You can manually download your results by running:\")\n",
        "        print(f\"!zip -r {prefix}_{timestamp}.zip {result_dir}\")\n",
        "        print(\"from google.colab import files\")\n",
        "        print(f\"files.download('{prefix}_{timestamp}.zip')\")\n",
        "        return False\n",
        "\n",
        "#################################################################################\n",
        "# PART 1: Test Sentences for Attention Analysis\n",
        "#################################################################################\n",
        "\n",
        "# Test sentences designed to probe different linguistic phenomena\n",
        "SIMPLE_SENTENCES = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog chased the ball across the yard.\",\n",
        "    \"She opened the door and walked into the room.\"\n",
        "]\n",
        "\n",
        "SYNTAX_COMPLEX = [\n",
        "    \"The student who studied hard for the exam passed with flying colors.\",\n",
        "    \"Although it was raining, they decided to go for a walk in the park.\",\n",
        "    \"If the company increases its profits, the shareholders will receive larger dividends.\"\n",
        "]\n",
        "\n",
        "LONG_RANGE_DEPENDENCIES = [\n",
        "    \"The keys, which I had left on the kitchen counter this morning, are now missing.\",\n",
        "    \"The CEO, despite objections from the board of directors, implemented the new policy.\",\n",
        "    \"The book that my professor recommended last semester is finally available in the library.\"\n",
        "]\n",
        "\n",
        "COREFERENCE_EXAMPLES = [\n",
        "    \"John said that he would finish the project by Friday.\",\n",
        "    \"After Mary graduated, she moved to New York to start her new job.\",\n",
        "    \"The engineers examined the bridge, and they determined it was unsafe.\"\n",
        "]\n",
        "\n",
        "NEGATION_EXAMPLES = [\n",
        "    \"The student did not complete the assignment on time.\",\n",
        "    \"No one believed that he could climb the mountain.\",\n",
        "    \"She hasn't visited Paris, but she plans to go next summer.\"\n",
        "]\n",
        "\n",
        "QUESTIONS = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"How do neural networks learn from data?\",\n",
        "    \"Why did the company's stock price fall yesterday?\"\n",
        "]\n",
        "\n",
        "TECHNICAL_CONTENT = [\n",
        "    \"Transformers use self-attention mechanisms to weigh the importance of different words in a sequence.\",\n",
        "    \"The gradient descent algorithm minimizes the loss function by iteratively updating the model parameters.\",\n",
        "    \"In computer vision, convolutional neural networks extract hierarchical features from input images.\"\n",
        "]\n",
        "\n",
        "AMBIGUOUS_SENTENCES = [\n",
        "    \"The professor lectured on the topic with enthusiasm.\",  # Who has enthusiasm?\n",
        "    \"They saw the mountains flying over the clouds.\",  # Who's flying?\n",
        "    \"The police arrested the protesters because they were violent.\"  # Who was violent?\n",
        "]\n",
        "\n",
        "COMPARATIVE_SENTENCES = [\n",
        "    \"This book is better than that one.\",\n",
        "    \"Programming in Python is easier than programming in C++.\",\n",
        "    \"The new model performs more efficiently than the older version.\"\n",
        "]\n",
        "\n",
        "TEMPORAL_SEQUENCES = [\n",
        "    \"First, preheat the oven, then prepare the ingredients, and finally bake for 30 minutes.\",\n",
        "    \"She woke up, brushed her teeth, took a shower, and then had breakfast.\",\n",
        "    \"Before signing the contract, make sure to read all the terms and conditions carefully.\"\n",
        "]\n",
        "\n",
        "ALL_TEST_CATEGORIES = {\n",
        "    \"Simple Sentences\": SIMPLE_SENTENCES,\n",
        "    \"Complex Syntax\": SYNTAX_COMPLEX,\n",
        "    \"Long-Range Dependencies\": LONG_RANGE_DEPENDENCIES,\n",
        "    \"Coreference Resolution\": COREFERENCE_EXAMPLES,\n",
        "    \"Negation\": NEGATION_EXAMPLES,\n",
        "    \"Questions\": QUESTIONS,\n",
        "    \"Technical Content\": TECHNICAL_CONTENT,\n",
        "    \"Ambiguous Sentences\": AMBIGUOUS_SENTENCES,\n",
        "    \"Comparisons\": COMPARATIVE_SENTENCES,\n",
        "    \"Temporal Sequences\": TEMPORAL_SEQUENCES\n",
        "}\n",
        "\n",
        "def get_test_sentences(categories=None, max_per_category=1):\n",
        "    \"\"\"\n",
        "    Get test sentences for attention analysis\n",
        "\n",
        "    Args:\n",
        "        categories: List of categories to include (None = all)\n",
        "        max_per_category: Maximum number of sentences per category\n",
        "\n",
        "    Returns:\n",
        "        List of test sentences\n",
        "    \"\"\"\n",
        "    test_sentences = []\n",
        "\n",
        "    if categories is None:\n",
        "        categories = list(ALL_TEST_CATEGORIES.keys())\n",
        "\n",
        "    for category in categories:\n",
        "        if category in ALL_TEST_CATEGORIES:\n",
        "            sentences = ALL_TEST_CATEGORIES[category]\n",
        "            selected = sentences[:max_per_category]\n",
        "            test_sentences.extend(selected)\n",
        "\n",
        "    return test_sentences\n",
        "\n",
        "#################################################################################\n",
        "# PART 2: Memory Management and Core Data Handling Functions\n",
        "#################################################################################\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clean up GPU memory\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def show_gpu_memory(context_message=\"Current\"):\n",
        "    \"\"\"Display GPU memory usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
        "        print(f\"{context_message} GPU memory: Allocated={allocated:.1f}MB, Reserved={reserved:.1f}MB\")\n",
        "\n",
        "class MixedSequenceDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for handling token sequences\"\"\"\n",
        "    def __init__(self, primary_sequences, secondary_sequences, p_primary: float):\n",
        "        super().__init__()\n",
        "        self.primary_sequences = primary_sequences if primary_sequences is not None else []\n",
        "        self.secondary_sequences = secondary_sequences if secondary_sequences is not None else []\n",
        "        self.p_primary = p_primary\n",
        "        self.has_primary = bool(self.primary_sequences)\n",
        "        self.has_secondary = bool(self.secondary_sequences)\n",
        "        if not self.has_primary and not self.has_secondary:\n",
        "            self.effective_length = 0\n",
        "        elif self.has_primary and (self.p_primary == 1.0 or not self.has_secondary):\n",
        "            self.effective_length = len(self.primary_sequences)\n",
        "        elif self.has_secondary and (self.p_primary == 0.0 or not self.has_primary):\n",
        "            self.effective_length = len(self.secondary_sequences)\n",
        "        elif self.has_primary and self.has_secondary:\n",
        "            self.effective_length = len(self.primary_sequences) + len(self.secondary_sequences)\n",
        "        else: self.effective_length = 0\n",
        "\n",
        "    def __len__(self): return self.effective_length\n",
        "    def __getitem__(self, idx):\n",
        "        if self.effective_length == 0: raise IndexError(\"Dataset is empty.\")\n",
        "        use_primary = self.has_primary and (not self.has_secondary or random.random() < self.p_primary)\n",
        "        chosen_list = self.primary_sequences if use_primary else self.secondary_sequences\n",
        "        if not chosen_list: raise IndexError(f\"Chosen list empty (primary: {use_primary}) despite non-zero effective_length.\")\n",
        "        actual_idx = idx % len(chosen_list)\n",
        "        return torch.tensor(chosen_list[actual_idx], dtype=torch.long)\n",
        "\n",
        "def seq_collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader\"\"\"\n",
        "    if not batch: return torch.tensor([], dtype=torch.long)\n",
        "    pad_token_id = 0\n",
        "    max_len = max(len(seq) for seq in batch)\n",
        "    padded = torch.full((max_len, len(batch)), pad_token_id, dtype=torch.long)\n",
        "    for i, seq in enumerate(batch): padded[:len(seq), i] = seq\n",
        "    return padded\n",
        "\n",
        "#################################################################################\n",
        "# PART 3: Experiment Configuration\n",
        "#################################################################################\n",
        "\n",
        "class ExperimentConfig:\n",
        "    \"\"\"Configuration for experiments\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        # Model architecture parameters\n",
        "        self.block_size = kwargs.get(\"block_size\", 384)\n",
        "        self.model_type = kwargs.get(\"model_type\", \"transformer\")\n",
        "        self.embed_size = kwargs.get(\"embed_size\", 448)\n",
        "        self.transformer_n_blocks = kwargs.get(\"transformer_n_blocks\", 8)\n",
        "        self.transformer_n_heads = kwargs.get(\"transformer_n_heads\", 7)  # Keep head_size divisible (448/7=64)\n",
        "        self.dropout_rate = kwargs.get(\"dropout_rate\", 0.1)  # Standard dropout\n",
        "\n",
        "        # Base pre-training parameters\n",
        "        self.base_dataset_name = kwargs.get(\"base_dataset_name\", \"wikitext\")\n",
        "        self.base_dataset_config = kwargs.get(\"base_dataset_config\", \"wikitext-103-v1\")\n",
        "        self.base_dataset_split = kwargs.get(\"base_dataset_split\", \"train[:20%]\")  # Use 20% of data to save time\n",
        "        self.base_num_epochs = kwargs.get(\"base_num_epochs\", 10)  # Fewer epochs for faster testing\n",
        "        self.base_learning_rate = kwargs.get(\"base_learning_rate\", 1e-4)\n",
        "        self.base_prompt = kwargs.get(\"base_prompt\", \"The meaning of life is\")  # Generic prompt\n",
        "\n",
        "        # General training parameters\n",
        "        self.batch_size = kwargs.get(\"batch_size\", 16)\n",
        "        self.max_steps_per_epoch = kwargs.get(\"max_steps_per_epoch\", None)\n",
        "        self.device_id = kwargs.get(\"device_id\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.log_interval_steps = kwargs.get(\"log_interval_steps\", 200)\n",
        "        self.sample_interval_seconds = kwargs.get(\"sample_interval_seconds\", 1200)\n",
        "\n",
        "        # Training stability/regularization parameters (primarily for pre-training)\n",
        "        self.gradient_accumulation_steps = kwargs.get(\"gradient_accumulation_steps\", 2)\n",
        "        self.weight_decay = kwargs.get(\"weight_decay\", 0.01)\n",
        "        self.warmup_ratio = kwargs.get(\"warmup_ratio\", 0.06)\n",
        "\n",
        "        # Model paths and saving\n",
        "        self.save_model = kwargs.get(\"save_model\", True)\n",
        "        self.base_model_save_path = kwargs.get(\"base_model_save_path\", \"results/base_transformer_for_interpretability.pt\")\n",
        "\n",
        "        self.experiment_name = kwargs.get(\"experiment_name\", f\"transformer_interpret_b{self.transformer_n_blocks}_h{self.transformer_n_heads}\")\n",
        "\n",
        "        # Parameters for Interpretability\n",
        "        self.interpret_model_path = kwargs.get(\"interpret_model_path\", self.base_model_save_path)\n",
        "        self.interpret_sample_sentences = kwargs.get(\"interpret_sample_sentences\", get_test_sentences(max_per_category=1))\n",
        "        self.interpret_layers_to_viz = kwargs.get(\"interpret_layers_to_viz\", [0, self.transformer_n_blocks // 2, self.transformer_n_blocks - 1])\n",
        "        self.interpret_heads_to_viz = kwargs.get(\"interpret_heads_to_viz\", [0, self.transformer_n_heads // 2])\n",
        "\n",
        "\n",
        "    def as_dict(self): return self.__dict__\n",
        "    def __str__(self): return json.dumps(self.as_dict(), indent=2)\n",
        "\n",
        "#################################################################################\n",
        "# PART 4: Experiment Tracker\n",
        "#################################################################################\n",
        "\n",
        "class ExperimentTracker:\n",
        "    \"\"\"Track experiment configurations and results\"\"\"\n",
        "    def __init__(self, root_dir=\"results\"):\n",
        "        self.root_dir = root_dir; self.experiments = {}; os.makedirs(self.root_dir, exist_ok=True)\n",
        "        print(f\"ExperimentTracker: Results in '{self.root_dir}'\")\n",
        "\n",
        "    def add_experiment_config(self, exp_name, cfg_obj):\n",
        "        if exp_name not in self.experiments: self.experiments[exp_name] = {\"training_history\":[], \"generated_samples\":[]}\n",
        "        self.experiments[exp_name][\"config_dict\"] = cfg_obj.as_dict() if isinstance(cfg_obj, ExperimentConfig) else cfg_obj\n",
        "        exp_dir = os.path.join(self.root_dir, exp_name); os.makedirs(exp_dir, exist_ok=True)\n",
        "        try:\n",
        "            with open(os.path.join(exp_dir, \"config.json\"),\"w\") as f: json.dump(self.experiments[exp_name][\"config_dict\"],f,indent=2)\n",
        "            print(f\"Tracker: Config for '{exp_name}' recorded.\")\n",
        "        except Exception as e: print(f\"Tracker: Error saving config for {exp_name}: {e}\")\n",
        "        return exp_name\n",
        "\n",
        "    def _ensure_exp(self, exp_name):\n",
        "        if exp_name not in self.experiments: self.add_experiment_config(exp_name, {\"name\":exp_name, \"auto\":True})\n",
        "\n",
        "    def log_training_step(self, exp_name, step, loss, glob_step=None):\n",
        "        self._ensure_exp(exp_name); self.experiments[exp_name][\"training_history\"].append({\"step\":step,\"global_step\":glob_step,\"loss\":loss})\n",
        "\n",
        "    def log_generated_sample(self, exp_name, p, gen_txt, method):\n",
        "        self._ensure_exp(exp_name); self.experiments[exp_name][\"generated_samples\"].append({\"prompt\":p,\"text\":gen_txt,\"method\":method,\"time\":time.time()})\n",
        "\n",
        "    def plot_training_history(self, exp_names=None, save=True):\n",
        "        if exp_names is None: exp_names=list(self.experiments.keys())\n",
        "        elif isinstance(exp_names,str): exp_names=[exp_names]\n",
        "        plt.figure(figsize=(12,6)); plotted=False\n",
        "        for name in exp_names:\n",
        "            hist = self.experiments.get(name,{}).get(\"training_history\",[])\n",
        "            if not hist: print(f\"Tracker: No history for '{name}'.\"); continue\n",
        "            steps = [e[\"global_step\"] if e.get(\"global_step\") is not None else e[\"step\"] for e in hist]\n",
        "            losses = [e[\"loss\"] for e in hist]\n",
        "            if steps and losses: plt.plot(steps,losses,label=name); plotted=True\n",
        "        if plotted:\n",
        "            plt.xlabel(\"Training Step\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss\"); plt.legend(); plt.grid(True,alpha=0.3)\n",
        "            if save: path=os.path.join(self.root_dir,\"all_training_loss.png\"); plt.savefig(path,dpi=300); print(f\"Tracker: Saved loss plot to {path}\")\n",
        "            plt.show()\n",
        "        else: print(\"Tracker: No data for training history plot.\")\n",
        "\n",
        "    def save_comparison_report(self, exp_names=None):\n",
        "        if exp_names is None: exp_names = list(self.experiments.keys())\n",
        "        elif isinstance(exp_names, str): exp_names = [exp_names]\n",
        "        report_path = os.path.join(self.root_dir, \"master_comparison_report.md\")\n",
        "        with open(report_path, \"w\") as f:\n",
        "            f.write(\"# Pico-LLM Master Experiment Comparison Report\\n\\n\")\n",
        "            for exp_name in exp_names:\n",
        "                exp_data = self.experiments.get(exp_name)\n",
        "                if not exp_data: continue\n",
        "                f.write(f\"## Experiment: {exp_name}\\n\\n\")\n",
        "                if \"config_dict\" in exp_data:\n",
        "                    f.write(\"### Configuration\\n```json\\n\" + json.dumps(exp_data[\"config_dict\"], indent=2) + \"\\n```\\n\\n\")\n",
        "                history = exp_data.get(\"training_history\", [])\n",
        "                if history:\n",
        "                    final_loss = history[-1][\"loss\"] if history else \"N/A\"\n",
        "                    num_steps = history[-1][\"global_step\"] if history and history[-1].get(\"global_step\") is not None else len(history)\n",
        "                    f.write(f\"### Training Summary\\n- Final Loss: {final_loss}\\n- Total Steps Logged: {num_steps}\\n\\n\")\n",
        "            print(f\"Tracker: Saved master comparison report to {report_path}\")\n",
        "\n",
        "#################################################################################\n",
        "# PART 5: Transformer Model Components (Modified for returning attention weights)\n",
        "#################################################################################\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"Layer Normalization\"\"\"\n",
        "    def __init__(self, features, eps=1e-5):\n",
        "        super().__init__(); self.weight=nn.Parameter(torch.ones(features)); self.bias=nn.Parameter(torch.zeros(features)); self.eps=eps\n",
        "    def forward(self, x):\n",
        "        mean=x.mean(-1,keepdim=True); std=x.std(-1,keepdim=True,unbiased=False); return self.weight*(x-mean)/(std+self.eps)+self.bias\n",
        "\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    \"\"\"Rotary Positional Embedding (RoPE)\"\"\"\n",
        "    def __init__(self, head_dim, max_seq_len_cache=2048):\n",
        "        super().__init__(); self.head_dim=head_dim; self.max_seq_len_cached=0\n",
        "        inv_freq=1.0/(10000**(torch.arange(0,head_dim,2).float()/head_dim))\n",
        "        self.register_buffer(\"inv_freq\",inv_freq,persistent=False); self._build_cache(max_seq_len_cache)\n",
        "\n",
        "    def _build_cache(self, seq_len_to_cache):\n",
        "        if seq_len_to_cache <= self.max_seq_len_cached and hasattr(self,'cos_cached'): return\n",
        "        self.max_seq_len_cached=seq_len_to_cache; t=torch.arange(self.max_seq_len_cached,device=self.inv_freq.device)\n",
        "        freqs=torch.einsum(\"i,j->ij\",t,self.inv_freq); emb=torch.cat((freqs,freqs),dim=-1)\n",
        "        if hasattr(self,'cos_cached'): del self.cos_cached\n",
        "        if hasattr(self,'sin_cached'): del self.sin_cached\n",
        "        self.register_buffer(\"cos_cached\",emb.cos(),persistent=False); self.register_buffer(\"sin_cached\",emb.sin(),persistent=False)\n",
        "\n",
        "    def _apply_rotary_emb(self, x, cos_val, sin_val):\n",
        "        cos=cos_val.unsqueeze(0).unsqueeze(0); sin=sin_val.unsqueeze(0).unsqueeze(0)\n",
        "        x1=x[...,0::2]; x2=x[...,1::2]; cos_use=cos[...,0::2]; sin_use=sin[...,0::2]\n",
        "        r_x1=x1*cos_use-x2*sin_use; r_x2=x1*sin_use+x2*cos_use\n",
        "        out=torch.zeros_like(x); out[...,0::2]=r_x1; out[...,1::2]=r_x2; return out\n",
        "\n",
        "    def forward(self, q_or_k, seq_len):\n",
        "        if seq_len > self.max_seq_len_cached: self._build_cache(seq_len)\n",
        "        cos_use=self.cos_cached[:seq_len,:self.head_dim].to(q_or_k.device)\n",
        "        sin_use=self.sin_cached[:seq_len,:self.head_dim].to(q_or_k.device)\n",
        "        return self._apply_rotary_emb(q_or_k,cos_use,sin_use)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention with option to return attention weights\"\"\"\n",
        "    def __init__(self, d_model, n_heads, block_size_for_rope_cache, dropout_rate=0.1):\n",
        "        super().__init__(); assert d_model%n_heads==0\n",
        "        self.d_m,self.n_h,self.h_dim = d_model,n_heads,d_model//n_heads\n",
        "        self.q_p=nn.Linear(d_model,d_model); self.k_p=nn.Linear(d_model,d_model)\n",
        "        self.v_p=nn.Linear(d_model,d_model); self.o_p=nn.Linear(d_model,d_model)\n",
        "        self.rope=RotaryPositionalEmbedding(self.h_dim,max_seq_len_cache=block_size_for_rope_cache)\n",
        "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
        "        self.resid_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, causal_mask=True, return_attention_weights=False):\n",
        "        b,s,_=x.shape\n",
        "        q=self.q_p(x).view(b,s,self.n_h,self.h_dim).transpose(1,2)\n",
        "        k=self.k_p(x).view(b,s,self.n_h,self.h_dim).transpose(1,2)\n",
        "        v=self.v_p(x).view(b,s,self.n_h,self.h_dim).transpose(1,2)\n",
        "        q=self.rope(q,s); k=self.rope(k,s)\n",
        "        scores=torch.matmul(q,k.transpose(-2,-1))/math.sqrt(self.h_dim)\n",
        "        if causal_mask:\n",
        "            mask=torch.triu(torch.ones(s,s,device=x.device,dtype=torch.bool),diagonal=1)\n",
        "            scores=scores.masked_fill(mask.unsqueeze(0).unsqueeze(0),float('-inf'))\n",
        "        attn_weights_softmaxed =F.softmax(scores,dim=-1)\n",
        "        attn_dropout_output = self.attn_dropout(attn_weights_softmaxed)\n",
        "        ctx=torch.matmul(attn_dropout_output,v).transpose(1,2).contiguous().view(b,s,self.d_m)\n",
        "        output = self.resid_dropout(self.o_p(ctx))\n",
        "        if return_attention_weights:\n",
        "            return output, attn_weights_softmaxed\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Feed-forward network with SwiGLU-like activation\"\"\"\n",
        "    def __init__(self, d_model, expansion_factor=4, dropout_rate=0.1):\n",
        "        super().__init__(); h_dim=int(d_model*expansion_factor)\n",
        "        self.w1=nn.Linear(d_model,h_dim,bias=False); self.w3=nn.Linear(d_model,h_dim,bias=False)\n",
        "        self.w2=nn.Linear(h_dim,d_model,bias=False); self.act=nn.SiLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "    def forward(self,x): return self.dropout(self.w2(self.act(self.w1(x))*self.w3(x)))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with option to return attention weights\"\"\"\n",
        "    def __init__(self, d_model, n_heads, block_size_for_rope_cache, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.n1=LayerNorm(d_model)\n",
        "        self.attn=MultiHeadAttention(d_model,n_heads,block_size_for_rope_cache, dropout_rate)\n",
        "        self.n2=LayerNorm(d_model)\n",
        "        self.ffn=FeedForward(d_model, dropout_rate=dropout_rate)\n",
        "\n",
        "    def forward(self,x, return_attention_weights=False):\n",
        "        attn_output = self.attn(self.n1(x), return_attention_weights=return_attention_weights)\n",
        "        if return_attention_weights:\n",
        "            attn_values, attn_weights = attn_output\n",
        "        else:\n",
        "            attn_values = attn_output\n",
        "\n",
        "        x = x + attn_values\n",
        "        x = x + self.ffn(self.n2(x))\n",
        "\n",
        "        if return_attention_weights:\n",
        "            return x, attn_weights\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Transformer language model with option to return attention weights\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, n_heads, n_blocks, block_size_for_pe, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.embed_dropout = nn.Dropout(dropout_rate)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [TransformerBlock(d_model, n_heads, block_size_for_pe, dropout_rate) for _ in range(n_blocks)]\n",
        "        )\n",
        "        self.norm_out = LayerNorm(d_model); self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, tokens_seq_batch_first, return_attention_weights=False):\n",
        "        x_embed = self.embed_dropout(self.token_embedding(tokens_seq_batch_first))\n",
        "\n",
        "        all_attention_weights = []\n",
        "        x_processed = x_embed\n",
        "        for block in self.blocks:\n",
        "            if return_attention_weights:\n",
        "                x_processed, attn_weights = block(x_processed, return_attention_weights=True)\n",
        "                all_attention_weights.append(attn_weights)\n",
        "            else:\n",
        "                x_processed = block(x_processed)\n",
        "\n",
        "        x_normed = self.norm_out(x_processed)\n",
        "        logits = self.lm_head(x_normed)\n",
        "\n",
        "        if return_attention_weights:\n",
        "            return logits, all_attention_weights\n",
        "        return logits\n",
        "\n",
        "#################################################################################\n",
        "# PART 6: Core Training & Generation Logic\n",
        "#################################################################################\n",
        "\n",
        "def compute_next_token_loss(logits_b_s_v, toks_b_s):\n",
        "    \"\"\"Compute cross entropy loss for next token prediction\"\"\"\n",
        "    if logits_b_s_v.size(1)<=1: return torch.tensor(0.0,device=logits_b_s_v.device,requires_grad=True)\n",
        "    return F.cross_entropy(logits_b_s_v[:,:-1,:].reshape(-1,logits_b_s_v.size(-1)), toks_b_s[:,1:].reshape(-1))\n",
        "\n",
        "def nucleus_sampling(logits_v, p=0.9, temp=0.8):\n",
        "    \"\"\"Sample from nucleus (top-p) with temperature\"\"\"\n",
        "    if temp==0: return torch.argmax(logits_v).item()\n",
        "    probs=F.softmax(logits_v/temp,dim=-1); s_probs,s_indices=torch.sort(probs,descending=True)\n",
        "    c_probs=torch.cumsum(s_probs,dim=-1); to_remove=c_probs>p\n",
        "    to_remove[...,1:]=to_remove[...,:-1].clone(); to_remove[...,0]=False\n",
        "    f_indices=s_indices[~to_remove]; f_probs=s_probs[~to_remove]\n",
        "    if f_probs.numel()==0: return torch.argmax(logits_v).item()\n",
        "    return f_indices[torch.multinomial(f_probs/torch.sum(f_probs),1)].item()\n",
        "\n",
        "def generate_text(model, enc, init_txt, max_new, dev, p=0.9, temp=0.8, use_greedy=False, verbose=False, cfg_for_gen=None):\n",
        "    \"\"\"Generate text using the model\"\"\"\n",
        "    model.eval(); ctx_ids=enc.encode(init_txt); gen_ids=[]\n",
        "    if verbose: print(f\"Prompt:\\\"{init_txt}\\\"\\nGen: \",end=\"\")\n",
        "    current_temp = temp; current_top_p = p\n",
        "    if use_greedy: current_temp = 0.0\n",
        "    curr_ids=list(ctx_ids); rept_count = 0; max_rept = 3\n",
        "    for _ in range(max_new):\n",
        "        block_size_to_use = cfg_for_gen.block_size if cfg_for_gen else model.blocks[0].attn.rope.max_seq_len_cached\n",
        "        input_ids_truncated = curr_ids[-block_size_to_use:]\n",
        "        in_tens=torch.tensor([input_ids_truncated],dtype=torch.long,device=dev)\n",
        "        if in_tens.size(1)==0: break\n",
        "        with torch.no_grad():\n",
        "            logits_all=model(in_tens); next_logits=logits_all[0,-1,:]\n",
        "            if use_greedy: next_id = torch.argmax(next_logits).item()\n",
        "            else: next_id=nucleus_sampling(next_logits,p=current_top_p,temp=current_temp)\n",
        "        if next_id==enc.eot_token:\n",
        "            if verbose: print(\"< EOT >\",end=\"\"); break\n",
        "        if gen_ids and next_id == gen_ids[-1]: rept_count += 1\n",
        "        else: rept_count = 0\n",
        "        if rept_count >= max_rept:\n",
        "            if verbose: print(\"<rep_stop>\",end=\"\"); break\n",
        "        gen_ids.append(next_id); curr_ids.append(next_id)\n",
        "        if verbose and (_+1)%10==0: print(\".\",end=\"\",flush=True)\n",
        "    if verbose: print(\"\\nDone.\")\n",
        "    full_txt=enc.decode(curr_ids); model.train()\n",
        "    return full_txt,gen_ids,[]\n",
        "\n",
        "def train_one_model(model, loader, epochs, log_prefix, dev, lr, log_steps, sample_secs, max_steps_epoch,\n",
        "                    encoder, sample_prompt, trk, exp_name, verbose=True,\n",
        "                    grad_accum_steps=1, warmup_ratio=0.1, weight_decay_val=0.01,\n",
        "                    early_stop_patience=100, cfg_for_gen_sampling=None):\n",
        "    \"\"\"Train or fine-tune a model\"\"\"\n",
        "    opt=optim.AdamW(model.parameters(),lr=lr,weight_decay=weight_decay_val,eps=1e-8)\n",
        "    num_batches_per_epoch = min(len(loader), max_steps_epoch) if max_steps_epoch is not None else len(loader)\n",
        "    total_training_steps = epochs * num_batches_per_epoch // grad_accum_steps\n",
        "    if total_training_steps == 0: total_training_steps = 1\n",
        "    warmup_steps = int(warmup_ratio * total_training_steps)\n",
        "    def lr_lambda(current_step_optimizer):\n",
        "        if current_step_optimizer < warmup_steps: return float(current_step_optimizer) / float(max(1, warmup_steps))\n",
        "        progress = float(current_step_optimizer - warmup_steps) / float(max(1, total_training_steps - warmup_steps))\n",
        "        return max(0.05, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    sched=optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "    t_start=time.time(); t_next_sample=t_start; glob_step=0; losses_plot=[]\n",
        "    best_epoch_loss = float('inf'); patience_counter = 0\n",
        "    print(f\"Train '{log_prefix}' {epochs}ep on {dev}. Eff.BatchSz:{loader.batch_size*grad_accum_steps}. LR(max):{lr}. Steps ~{total_training_steps}. Warmup:{warmup_steps}. Exp:'{exp_name}'\")\n",
        "    for ep in range(1,epochs+1):\n",
        "        model.train(); ep_loss=0.0; ep_steps=0\n",
        "        eff_steps_ep = num_batches_per_epoch\n",
        "        if eff_steps_ep == 0: print(f\"Warn: Dataloader for {exp_name} ep {ep} empty.\"); continue\n",
        "        bar=tqdm(total=eff_steps_ep,desc=f\"Ep {ep}/{epochs} (Exp:{exp_name})\",leave=True)\n",
        "        for batch_idx, batch_time_maj in enumerate(loader):\n",
        "            if max_steps_epoch and ep_steps >= max_steps_epoch: break\n",
        "            if (batch_idx % grad_accum_steps == 0): opt.zero_grad(set_to_none=True)\n",
        "            batch_b_s = batch_time_maj.transpose(0,1).to(dev)\n",
        "            try:\n",
        "                logits_b_s_v=model(batch_b_s); loss=compute_next_token_loss(logits_b_s_v,batch_b_s)\n",
        "                loss_scaled = loss / grad_accum_steps; loss_scaled.backward()\n",
        "                if (batch_idx + 1) % grad_accum_steps == 0 or (batch_idx + 1) == len(loader):\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0); opt.step(); sched.step()\n",
        "                lv=loss.item(); ep_loss+=lv; losses_plot.append(lv); ep_steps+=1; glob_step+=1\n",
        "                if trk: trk.log_training_step(exp_name,ep_steps,lv,glob_step)\n",
        "                current_lr_display = opt.param_groups[0]['lr']\n",
        "                bar.update(1); bar.set_postfix({\"loss\":f\"{lv:.4f}\",\"lr\":f\"{current_lr_display:.2e}\"})\n",
        "                if verbose and glob_step%log_steps==0:\n",
        "                    print(f\"\\n[{log_prefix}] Ep {ep},St {ep_steps}/{eff_steps_ep}(Glob:{glob_step})|Loss:{lv:.4f}|LR:{current_lr_display:.2e}\")\n",
        "                    if torch.cuda.is_available(): show_gpu_memory(f\"St {glob_step}\")\n",
        "                if encoder and sample_prompt and trk and time.time()>=t_next_sample:\n",
        "                    if verbose: print(f\"\\n[{log_prefix}] Sampling...\"); clear_gpu_memory()\n",
        "                    try:\n",
        "                        gen_p = 0.9\n",
        "                        gen_temp = 0.8\n",
        "                        gen_greedy = False\n",
        "\n",
        "                        s_txt,_,_=generate_text(model,encoder,sample_prompt,50,dev,\n",
        "                                                p=gen_p, temp=gen_temp, use_greedy=gen_greedy,\n",
        "                                                verbose=False, cfg_for_gen=cfg_for_gen_sampling)\n",
        "                        if verbose: print(f\"Sample:{s_txt}\"); trk.log_generated_sample(exp_name,sample_prompt,s_txt,\"nucl_config_sample\")\n",
        "                    except Exception as e: print(f\"Warn: Sample gen err: {e}\")\n",
        "                    t_next_sample=time.time()+sample_secs\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e).lower(): print(\"OOM ERR. Skip batch.\"); clear_gpu_memory(); opt.zero_grad(set_to_none=True); continue\n",
        "                else: raise e\n",
        "        bar.close(); avg_ep_loss=ep_loss/ep_steps if ep_steps>0 else float('nan')\n",
        "        print(f\"[{log_prefix}] Ep {ep} done. Avg Loss:{avg_ep_loss:.4f}\")\n",
        "        if 'FINETUNE' in exp_name:\n",
        "            if avg_ep_loss < best_epoch_loss:\n",
        "                best_epoch_loss = avg_ep_loss; patience_counter = 0\n",
        "                if trk:\n",
        "                    chkpt_path = os.path.join(trk.root_dir, exp_name, \"best_model_checkpoint.pt\")\n",
        "                    os.makedirs(os.path.dirname(chkpt_path), exist_ok=True)\n",
        "                    torch.save(model.state_dict(), chkpt_path); print(f\"Saved best fine-tune checkpoint to {chkpt_path} (Loss: {best_epoch_loss:.4f})\")\n",
        "            else: patience_counter += 1\n",
        "            if patience_counter >= early_stop_patience: print(f\"Early stopping at epoch {ep} for {exp_name}. Best loss: {best_epoch_loss:.4f}\"); break\n",
        "        clear_gpu_memory()\n",
        "    if 'FINETUNE' in exp_name and trk and os.path.exists(os.path.join(trk.root_dir, exp_name, \"best_model_checkpoint.pt\")):\n",
        "        print(f\"Loading best fine-tuned model checkpoint for {exp_name} (Loss: {best_epoch_loss:.4f})...\")\n",
        "        model.load_state_dict(torch.load(os.path.join(trk.root_dir, exp_name, \"best_model_checkpoint.pt\"), map_location=dev))\n",
        "    if losses_plot and trk:\n",
        "        plt.figure(figsize=(10,5)); plt.plot(losses_plot); plt.title(f\"Loss:{log_prefix}(Exp:{exp_name})\")\n",
        "        plt.xlabel(\"Training Batches (Accumulated)\"); plt.ylabel(\"Loss\"); plt.grid(True,alpha=0.3)\n",
        "        l_path=os.path.join(trk.root_dir,exp_name,\"run_loss_curve.png\"); os.makedirs(os.path.dirname(l_path),exist_ok=True)\n",
        "        plt.savefig(l_path); print(f\"Saved run loss curve to {l_path}\"); plt.show()\n",
        "\n",
        "        # In Colab, download checkpoint at this point\n",
        "        if IN_COLAB and model and trk:\n",
        "            # Save model checkpoint\n",
        "            model_path = os.path.join(trk.root_dir, exp_name, \"model_checkpoint_latest.pt\")\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(f\"Saved latest model checkpoint to {model_path}\")\n",
        "            # Download checkpoint\n",
        "            auto_download_results(trk.root_dir, f\"pico_llm_checkpoint_{exp_name}\")\n",
        "\n",
        "    return avg_ep_loss if epochs>0 else float('nan')\n",
        "\n",
        "#################################################################################\n",
        "# PART 7: Experiment Runner Functions\n",
        "#################################################################################\n",
        "\n",
        "def train_base_transformer(config, current_tracker, current_encoder):\n",
        "    \"\"\"Pre-train a base transformer model\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80 + f\"\\nSTARTING BASE PRE-TRAINING on {config.base_dataset_name} (Concatenated)\\n\" + \"=\"*80)\n",
        "    clear_gpu_memory(); show_gpu_memory(\"Before base data load\")\n",
        "    ds_id, ds_conf, ds_split = config.base_dataset_name, config.base_dataset_config, config.base_dataset_split\n",
        "    print(f\"Loading & Concatenating: '{ds_id}' (config:'{ds_conf}'), split:'{ds_split}'\")\n",
        "    try:\n",
        "        base_ds = load_dataset(ds_id, name=ds_conf, split=ds_split)\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal: Error loading base dataset: {e}. Aborting.\"); return None\n",
        "    vocab_size = current_encoder.n_vocab; all_tokens = []\n",
        "    print(\"Tokenizing and concatenating dataset...\")\n",
        "    for item in tqdm(base_ds, desc=f\"Tokenizing {ds_id}\"):\n",
        "        text = item.get('text','').strip()\n",
        "        if not text: continue\n",
        "        tokens = current_encoder.encode(text); all_tokens.extend(tokens)\n",
        "        all_tokens.append(current_encoder.eot_token)\n",
        "    if not all_tokens: print(\"No tokens from dataset. Aborting.\"); return None\n",
        "    sequences = [all_tokens[i:i+config.block_size] for i in range(0,len(all_tokens)-config.block_size+1,config.block_size)]\n",
        "    if not sequences: print(f\"No sequences of block_size {config.block_size}. Aborting.\"); return None\n",
        "    print(f\"Created {len(sequences)} sequences for pre-training from concatenated {ds_id}.\")\n",
        "    dataset = MixedSequenceDataset(sequences, [], 1.0)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size, shuffle=True, num_workers=0, collate_fn=seq_collate_fn, pin_memory=False)\n",
        "    device = torch.device(config.device_id)\n",
        "    model = TransformerModel(vocab_size, config.embed_size, config.transformer_n_heads, config.transformer_n_blocks, config.block_size, config.dropout_rate).to(device)\n",
        "    exp_name = f\"PRETRAIN_CONCAT_{ds_id.replace('-','_')}_{ds_conf.replace('-','_') if ds_conf else 'nocfg'}_e{config.embed_size}_b{config.transformer_n_blocks}_lr{config.base_learning_rate}\"\n",
        "    current_tracker.add_experiment_config(exp_name, config)\n",
        "    print(f\"\\nStarting base pre-train (Exp:{exp_name}). {len(loader)} batches/epoch.\"); show_gpu_memory(\"Before base train loop\")\n",
        "    train_one_model(model, loader, config.base_num_epochs, exp_name, device, config.base_learning_rate,\n",
        "                    config.log_interval_steps, config.sample_interval_seconds, config.max_steps_per_epoch,\n",
        "                    current_encoder, config.base_prompt, current_tracker, exp_name,\n",
        "                    grad_accum_steps=config.gradient_accumulation_steps,\n",
        "                    warmup_ratio=config.warmup_ratio,\n",
        "                    weight_decay_val=config.weight_decay,\n",
        "                    early_stop_patience=100,\n",
        "                    cfg_for_gen_sampling=config)\n",
        "    if config.save_model:\n",
        "        path = config.base_model_save_path; p_dir = os.path.dirname(path)\n",
        "        if p_dir: os.makedirs(p_dir, exist_ok=True)\n",
        "        torch.save(model.state_dict(), path); print(f\"Saved base model to '{path}'\")\n",
        "\n",
        "        # In Colab, download the saved model\n",
        "        if IN_COLAB:\n",
        "            auto_download_results(p_dir, \"pico_llm_base_model\")\n",
        "\n",
        "    clear_gpu_memory(); show_gpu_memory(\"After base pre-train\")\n",
        "    return model\n",
        "\n",
        "#################################################################################\n",
        "# PART 8: Attention Visualization Functions\n",
        "#################################################################################\n",
        "\n",
        "def create_visualization_dir(base_dir, model_name=None):\n",
        "    \"\"\"Create directory structure for visualizations\"\"\"\n",
        "    if model_name:\n",
        "        viz_dir = os.path.join(base_dir, \"attention_visualizations\", model_name)\n",
        "    else:\n",
        "        viz_dir = os.path.join(base_dir, \"attention_visualizations\")\n",
        "\n",
        "    # Create subdirectories for different visualization types\n",
        "    os.makedirs(os.path.join(viz_dir, \"heatmaps\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(viz_dir, \"patterns\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(viz_dir, \"graphs\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(viz_dir, \"summaries\"), exist_ok=True)\n",
        "\n",
        "    return viz_dir\n",
        "\n",
        "def clean_token_for_display(token, encoder):\n",
        "    \"\"\"Clean token for display in visualizations\"\"\"\n",
        "    if not token or token.strip() == '':\n",
        "        return \"[PAD]\"\n",
        "    if token == encoder.decode([encoder.eot_token]):\n",
        "        return \"[EOT]\"\n",
        "    # Replace newlines for display\n",
        "    token = token.replace('\\n', '\\\\n')\n",
        "    if len(token) > 10:\n",
        "        token = token[:8] + \"...\"\n",
        "    return token\n",
        "\n",
        "def plot_attention_heatmap(tokens, attention_weights, layer_idx, head_idx, save_dir, sentence_name=\"sentence\", max_len=30):\n",
        "    \"\"\"\n",
        "    Enhanced heatmap for attention visualization\n",
        "    \"\"\"\n",
        "    if len(tokens) > max_len:\n",
        "        print(f\"Truncating tokens from {len(tokens)} to {max_len} for visualization\")\n",
        "        tokens = tokens[:max_len]\n",
        "        attention_weights = attention_weights[:max_len, :max_len]\n",
        "\n",
        "    # Create a custom colormap from white to dark blue\n",
        "    colors = [(1, 1, 1), (0.8, 0.8, 1), (0.5, 0.5, 0.8), (0.2, 0.2, 0.5), (0, 0, 0.3)]\n",
        "    cmap_name = 'white_to_blue'\n",
        "    cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)\n",
        "\n",
        "    # Determine grid size based on token count\n",
        "    fig_size = min(12, max(8, len(tokens) * 0.4))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
        "\n",
        "    # Plot heatmap with seaborn for better aesthetics\n",
        "    sns.heatmap(\n",
        "        attention_weights.cpu().numpy(),\n",
        "        annot=False,\n",
        "        cmap=cm,\n",
        "        square=True,\n",
        "        xticklabels=tokens,\n",
        "        yticklabels=tokens,\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    # Customize appearance\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontsize=8)\n",
        "    plt.setp(ax.get_yticklabels(), rotation=0, fontsize=8)\n",
        "\n",
        "    # Add title with more info\n",
        "    ax.set_title(f\"Attention: Layer {layer_idx+1}, Head {head_idx+1}\", fontsize=12)\n",
        "\n",
        "    # Add source/target labels\n",
        "    ax.set_xlabel(\"Attention To (Key)\", fontsize=10)\n",
        "    ax.set_ylabel(\"Attention From (Query)\", fontsize=10)\n",
        "\n",
        "    # Make sure the plot is tight\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # Save the plot\n",
        "    filename = f\"heatmap_L{layer_idx+1}_H{head_idx+1}_{sentence_name}.png\"\n",
        "    filepath = os.path.join(save_dir, \"heatmaps\", filename)\n",
        "    plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
        "\n",
        "    # Return the filepath for reference\n",
        "    return filepath\n",
        "\n",
        "def extract_attention_patterns(tokens, all_layer_attn_weights):\n",
        "    \"\"\"\n",
        "    Extract meaningful patterns from attention weights\n",
        "    \"\"\"\n",
        "    n_layers = len(all_layer_attn_weights)\n",
        "    n_heads = all_layer_attn_weights[0].size(1)\n",
        "    seq_len = len(tokens)\n",
        "\n",
        "    pattern_data = {\n",
        "        \"local\": [],       # Attends mostly to adjacent tokens\n",
        "        \"diagonal\": [],    # Attends to self or previous token\n",
        "        \"global\": [],      # Attends broadly across the sequence\n",
        "        \"token_specific\": [] # Attends to specific token types\n",
        "    }\n",
        "\n",
        "    # Detailed metrics for each head\n",
        "    head_metrics = {}\n",
        "\n",
        "    for layer_idx in range(n_layers):\n",
        "        layer_weights = all_layer_attn_weights[layer_idx]\n",
        "\n",
        "        for head_idx in range(n_heads):\n",
        "            head_weights = layer_weights[0, head_idx].cpu().numpy()  # (seq_len, seq_len)\n",
        "\n",
        "            # Calculate metrics\n",
        "            diagonal_weight = np.mean(np.diag(head_weights))\n",
        "            adjacent_weight = 0\n",
        "            for i in range(seq_len-1):\n",
        "                adjacent_weight += (head_weights[i, i+1] + head_weights[i+1, i])\n",
        "            adjacent_weight /= (2 * (seq_len-1)) if seq_len > 1 else 1\n",
        "\n",
        "            # Find top attended tokens\n",
        "            top_attended = []\n",
        "            for i in range(seq_len):\n",
        "                max_idx = np.argmax(head_weights[i])\n",
        "                if max_idx != i and head_weights[i, max_idx] > 0.3:  # Significant attention\n",
        "                    top_attended.append((i, max_idx, tokens[i], tokens[max_idx], head_weights[i, max_idx]))\n",
        "\n",
        "            # Calculate entropy to measure attention spread\n",
        "            eps = 1e-10  # To avoid log(0)\n",
        "            entropy = -np.sum(head_weights * np.log(head_weights + eps)) / seq_len\n",
        "\n",
        "            # Categorize head behavior\n",
        "            if diagonal_weight > 0.5 or adjacent_weight > 0.4:\n",
        "                pattern_data[\"diagonal\"].append((layer_idx, head_idx, diagonal_weight, adjacent_weight))\n",
        "            elif adjacent_weight > 0.3:\n",
        "                pattern_data[\"local\"].append((layer_idx, head_idx, adjacent_weight))\n",
        "            elif entropy < 0.3 * np.log(seq_len):  # Low entropy = focused attention\n",
        "                pattern_data[\"token_specific\"].append((layer_idx, head_idx, entropy, top_attended))\n",
        "            else:\n",
        "                pattern_data[\"global\"].append((layer_idx, head_idx, entropy))\n",
        "\n",
        "            # Store metrics for this head\n",
        "            head_metrics[(layer_idx, head_idx)] = {\n",
        "                \"diagonal_weight\": diagonal_weight,\n",
        "                \"adjacent_weight\": adjacent_weight,\n",
        "                \"entropy\": entropy,\n",
        "                \"top_attended\": top_attended\n",
        "            }\n",
        "\n",
        "    return pattern_data, head_metrics\n",
        "\n",
        "def visualize_attention_patterns(pattern_data, n_layers, n_heads, save_dir):\n",
        "    \"\"\"\n",
        "    Create visualizations of attention pattern distributions across heads\n",
        "    \"\"\"\n",
        "    # Create grid showing which heads have which patterns\n",
        "    pattern_grid = np.zeros((n_layers, n_heads, 4))  # 4 pattern types\n",
        "\n",
        "    # Map patterns to grid\n",
        "    pattern_types = [\"diagonal\", \"local\", \"global\", \"token_specific\"]\n",
        "    pattern_names = [\"Diagonal/Self\", \"Local Context\", \"Global/Broad\", \"Token-Specific\"]\n",
        "\n",
        "    for i, pattern_type in enumerate(pattern_types):\n",
        "        for data in pattern_data[pattern_type]:\n",
        "            layer_idx, head_idx = data[0], data[1]\n",
        "            pattern_grid[layer_idx, head_idx, i] = 1\n",
        "\n",
        "    # Plotting\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i, (pattern_type, pattern_name) in enumerate(zip(pattern_types, pattern_names)):\n",
        "        ax = axs[i]\n",
        "        im = ax.imshow(pattern_grid[:, :, i], cmap='Blues', vmin=0, vmax=1)\n",
        "        ax.set_title(f\"{pattern_name} Attention Heads\", fontsize=12)\n",
        "        ax.set_xlabel(\"Head Index\")\n",
        "        ax.set_ylabel(\"Layer Index\")\n",
        "\n",
        "        # Set ticks\n",
        "        ax.set_xticks(np.arange(n_heads))\n",
        "        ax.set_yticks(np.arange(n_layers))\n",
        "        ax.set_xticklabels([f\"H{h+1}\" for h in range(n_heads)])\n",
        "        ax.set_yticklabels([f\"L{l+1}\" for l in range(n_layers)])\n",
        "\n",
        "        # Add text annotations\n",
        "        for l in range(n_layers):\n",
        "            for h in range(n_heads):\n",
        "                if pattern_grid[l, h, i] > 0:\n",
        "                    ax.text(h, l, \"✓\", ha=\"center\", va=\"center\", color=\"black\", fontweight=\"bold\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filepath = os.path.join(save_dir, \"patterns\", \"attention_pattern_distribution.png\")\n",
        "    plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Pie chart of pattern distribution\n",
        "    pattern_counts = [len(pattern_data[pt]) for pt in pattern_types]\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.pie(\n",
        "        pattern_counts,\n",
        "        labels=pattern_names,\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=90,\n",
        "        colors=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
        "    )\n",
        "    plt.title('Distribution of Attention Head Types', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filepath = os.path.join(save_dir, \"patterns\", \"pattern_distribution_pie.png\")\n",
        "    plt.savefig(filepath, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def plot_attention_flow_graph(tokens, attention_weights, layer_idx, head_idx, save_dir, sentence_name=\"sentence\", min_weight=0.15):\n",
        "    \"\"\"\n",
        "    Create graph visualization showing attention flow between tokens\n",
        "    \"\"\"\n",
        "    seq_len = len(tokens)\n",
        "    attn = attention_weights.cpu().numpy()\n",
        "\n",
        "    # Create graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes\n",
        "    for i in range(seq_len):\n",
        "        G.add_node(i, label=tokens[i])\n",
        "\n",
        "    # Add edges (attention connections) above threshold\n",
        "    for i in range(seq_len):\n",
        "        for j in range(seq_len):\n",
        "            if attn[i, j] > min_weight:\n",
        "                G.add_edge(i, j, weight=float(attn[i, j]), penwidth=attn[i, j] * 10)\n",
        "\n",
        "    if len(G.edges) == 0:\n",
        "        print(f\"No edges above threshold {min_weight} for Layer {layer_idx+1}, Head {head_idx+1}\")\n",
        "        return None\n",
        "\n",
        "    # Set up plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Use spring layout with some modifications\n",
        "    pos = nx.spring_layout(G, k=0.8, iterations=100, seed=42)\n",
        "\n",
        "    # Draw nodes\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue', alpha=0.8)\n",
        "\n",
        "    # Draw edges with width based on attention weight\n",
        "    edges = G.edges(data='weight')\n",
        "    weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
        "\n",
        "    # Normalize weights for better visualization\n",
        "    norm_weights = [w / max(weights) for w in weights]\n",
        "\n",
        "    # Draw edges\n",
        "    nx.draw_networkx_edges(\n",
        "        G, pos,\n",
        "        width=[w * 3 for w in norm_weights],\n",
        "        alpha=[w * 0.8 + 0.2 for w in norm_weights],\n",
        "        edge_color='gray',\n",
        "        arrows=True,\n",
        "        arrowsize=15,\n",
        "        arrowstyle='->'\n",
        "    )\n",
        "\n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n",
        "\n",
        "    plt.title(f\"Attention Flow: Layer {layer_idx+1}, Head {head_idx+1}\", fontsize=14)\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Save figure\n",
        "    filename = f\"flow_L{layer_idx+1}_H{head_idx+1}_{sentence_name}.png\"\n",
        "    filepath = os.path.join(save_dir, \"graphs\", filename)\n",
        "    plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def create_head_similarity_matrix(all_sentences_metrics):\n",
        "    \"\"\"\n",
        "    Create a similarity matrix between heads based on their behavior across sentences\n",
        "    \"\"\"\n",
        "    # Extract all unique head indices\n",
        "    all_heads = set()\n",
        "    for metrics in all_sentences_metrics:\n",
        "        all_heads.update(metrics.keys())\n",
        "\n",
        "    all_heads = sorted(list(all_heads))\n",
        "    n_heads = len(all_heads)\n",
        "\n",
        "    # Features to consider for similarity\n",
        "    features = ['diagonal_weight', 'adjacent_weight', 'entropy']\n",
        "\n",
        "    # Initialize feature vectors for each head\n",
        "    head_features = {head: [] for head in all_heads}\n",
        "\n",
        "    # Collect features across all sentences\n",
        "    for metrics in all_sentences_metrics:\n",
        "        for head in all_heads:\n",
        "            if head in metrics:\n",
        "                head_features[head].extend([\n",
        "                    metrics[head][feature] for feature in features\n",
        "                ])\n",
        "            else:\n",
        "                # Use default values if head wasn't analyzed for this sentence\n",
        "                head_features[head].extend([0.0, 0.0, 1.0])  # Default values\n",
        "\n",
        "    # Convert to feature matrix\n",
        "    feature_matrix = np.array([head_features[head] for head in all_heads])\n",
        "\n",
        "    # Compute similarity matrix (cosine similarity)\n",
        "    similarity_matrix = np.zeros((n_heads, n_heads))\n",
        "    for i in range(n_heads):\n",
        "        for j in range(n_heads):\n",
        "            # Calculate cosine similarity\n",
        "            dot_product = np.dot(feature_matrix[i], feature_matrix[j])\n",
        "            norm_i = np.linalg.norm(feature_matrix[i])\n",
        "            norm_j = np.linalg.norm(feature_matrix[j])\n",
        "            if norm_i > 0 and norm_j > 0:\n",
        "                similarity_matrix[i, j] = dot_product / (norm_i * norm_j)\n",
        "            else:\n",
        "                similarity_matrix[i, j] = 0\n",
        "\n",
        "    return similarity_matrix, all_heads\n",
        "\n",
        "def plot_head_similarity_matrix(similarity_matrix, all_heads, save_dir):\n",
        "    \"\"\"\n",
        "    Plot the similarity matrix between heads as a heatmap\n",
        "    \"\"\"\n",
        "    # Extract layer/head info\n",
        "    head_labels = [f\"L{h[0]+1}H{h[1]+1}\" for h in all_heads]\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(\n",
        "        similarity_matrix,\n",
        "        annot=False,\n",
        "        xticklabels=head_labels,\n",
        "        yticklabels=head_labels,\n",
        "        cmap='viridis'\n",
        "    )\n",
        "    plt.title(\"Attention Head Similarity Matrix\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filepath = os.path.join(save_dir, \"summaries\", \"head_similarity_matrix.png\")\n",
        "    plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def cluster_attention_heads(similarity_matrix, all_heads, save_dir):\n",
        "    \"\"\"\n",
        "    Cluster attention heads based on similarity and visualize the hierarchy\n",
        "    \"\"\"\n",
        "    # Compute linkage matrix for hierarchical clustering\n",
        "    linkage_matrix = linkage(1 - similarity_matrix, method='ward')\n",
        "\n",
        "    # Extract layer/head info\n",
        "    head_labels = [f\"L{h[0]+1}H{h[1]+1}\" for h in all_heads]\n",
        "\n",
        "    plt.figure(figsize=(18, 10))\n",
        "\n",
        "    # Plot dendrogram\n",
        "    dendrogram(\n",
        "        linkage_matrix,\n",
        "        labels=head_labels,\n",
        "        leaf_rotation=90.,\n",
        "        color_threshold=0.7,  # Adjust this threshold for cluster coloring\n",
        "    )\n",
        "\n",
        "    plt.title(\"Hierarchical Clustering of Attention Heads\", fontsize=14)\n",
        "    plt.xlabel(\"Attention Heads\", fontsize=12)\n",
        "    plt.ylabel(\"Distance\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filepath = os.path.join(save_dir, \"summaries\", \"head_clustering_dendrogram.png\")\n",
        "    plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def create_attention_behavior_summary(all_sentences_data, save_dir):\n",
        "    \"\"\"\n",
        "    Create a detailed summary of attention head behaviors\n",
        "    \"\"\"\n",
        "    # Collect pattern data across sentences\n",
        "    head_pattern_counts = defaultdict(lambda: {\n",
        "        \"diagonal\": 0, \"local\": 0, \"global\": 0, \"token_specific\": 0, \"total\": 0\n",
        "    })\n",
        "\n",
        "    for sentence_idx, (_, pattern_data, _) in enumerate(all_sentences_data):\n",
        "        for pattern_type in [\"diagonal\", \"local\", \"global\", \"token_specific\"]:\n",
        "            for data in pattern_data[pattern_type]:\n",
        "                head = (data[0], data[1])  # (layer_idx, head_idx)\n",
        "                head_pattern_counts[head][pattern_type] += 1\n",
        "                head_pattern_counts[head][\"total\"] += 1\n",
        "\n",
        "    # Determine primary behavior for each head\n",
        "    head_behaviors = {}\n",
        "    for head, counts in head_pattern_counts.items():\n",
        "        if counts[\"total\"] == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate percentages\n",
        "        percentages = {\n",
        "            pattern: count / counts[\"total\"] * 100\n",
        "            for pattern, count in counts.items() if pattern != \"total\"\n",
        "        }\n",
        "\n",
        "        # Determine primary behavior (pattern with highest percentage)\n",
        "        primary_pattern = max(percentages.items(), key=lambda x: x[1])\n",
        "\n",
        "        head_behaviors[head] = {\n",
        "            \"primary_pattern\": primary_pattern[0],\n",
        "            \"primary_percentage\": primary_pattern[1],\n",
        "            \"all_percentages\": percentages,\n",
        "            \"sample_count\": counts[\"total\"]\n",
        "        }\n",
        "\n",
        "    # Create a DataFrame for visualization\n",
        "    df_data = []\n",
        "    for head, behavior in head_behaviors.items():\n",
        "        layer_idx, head_idx = head\n",
        "        df_data.append({\n",
        "            \"Layer\": layer_idx + 1,\n",
        "            \"Head\": head_idx + 1,\n",
        "            \"Primary Behavior\": behavior[\"primary_pattern\"].capitalize(),\n",
        "            \"Confidence\": behavior[\"primary_percentage\"],\n",
        "            \"Sample Count\": behavior[\"sample_count\"]\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df_data)\n",
        "\n",
        "    # Sort by layer and head\n",
        "    df = df.sort_values([\"Layer\", \"Head\"])\n",
        "\n",
        "    # Create a summary visualization\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Use different colors for different behaviors\n",
        "    behavior_colors = {\n",
        "        \"Diagonal\": \"#ff9999\",\n",
        "        \"Local\": \"#66b3ff\",\n",
        "        \"Global\": \"#99ff99\",\n",
        "        \"Token_specific\": \"#ffcc99\"\n",
        "    }\n",
        "\n",
        "    # Group by layer\n",
        "    grouped = df.groupby(\"Layer\")\n",
        "\n",
        "    # Create a grid of bars, one row per layer\n",
        "    n_layers = len(grouped)\n",
        "\n",
        "    fig, axs = plt.subplots(n_layers, 1, figsize=(12, n_layers * 2.5), sharex=True)\n",
        "\n",
        "    for (layer, layer_df), ax in zip(grouped, axs if n_layers > 1 else [axs]):\n",
        "        # Create bars for each head\n",
        "        bars = ax.bar(\n",
        "            layer_df[\"Head\"],\n",
        "            layer_df[\"Confidence\"],\n",
        "            color=[behavior_colors.get(b.replace(\"-\", \"_\"), \"#cccccc\") for b in layer_df[\"Primary Behavior\"]]\n",
        "        )\n",
        "\n",
        "        # Add labels\n",
        "        ax.set_title(f\"Layer {layer} Head Behaviors\", fontsize=11)\n",
        "        ax.set_ylim(0, 110)  # Leave room for text\n",
        "\n",
        "        # Add behavior text on each bar\n",
        "        for bar, behavior in zip(bars, layer_df[\"Primary Behavior\"]):\n",
        "            ax.text(\n",
        "                bar.get_x() + bar.get_width()/2,\n",
        "                bar.get_height() + 5,\n",
        "                behavior,\n",
        "                ha='center',\n",
        "                va='bottom',\n",
        "                rotation=90,\n",
        "                fontsize=9\n",
        "            )\n",
        "\n",
        "    # Add a single legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor=color, label=behavior)\n",
        "        for behavior, color in behavior_colors.items()\n",
        "    ]\n",
        "    fig.legend(\n",
        "        handles=legend_elements,\n",
        "        loc='lower center',\n",
        "        bbox_to_anchor=(0.5, 0),\n",
        "        ncol=len(behavior_colors),\n",
        "        frameon=False\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.1)  # Make room for the legend\n",
        "\n",
        "    filepath = os.path.join(save_dir, \"summaries\", \"head_behavior_summary.png\")\n",
        "    plt.savefig(filepath, dpi=200, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Also create a table for the markdown report\n",
        "    table_path = os.path.join(save_dir, \"summaries\", \"head_behavior_table.csv\")\n",
        "    df.to_csv(table_path, index=False)\n",
        "\n",
        "    return filepath, table_path\n",
        "\n",
        "def create_interpretability_report(all_sentences_data, model_name, viz_dir, encoder):\n",
        "    \"\"\"\n",
        "    Create a comprehensive markdown report of the interpretability analysis\n",
        "    \"\"\"\n",
        "    # Extract information\n",
        "    sentences = [data[0] for data in all_sentences_data]\n",
        "    all_pattern_data = [data[1] for data in all_sentences_data]\n",
        "    all_head_metrics = [data[2] for data in all_sentences_data]\n",
        "\n",
        "    # Create report content\n",
        "    report = [\n",
        "        f\"# Transformer Interpretability Analysis: {model_name}\",\n",
        "        \"\",\n",
        "        \"## Overview\",\n",
        "        \"\",\n",
        "        f\"This report analyzes the attention patterns in a transformer model with the following architecture:\",\n",
        "        \"\",\n",
        "        \"- Embedding size: 448\",\n",
        "        \"- Number of layers: 8\",\n",
        "        \"- Number of attention heads per layer: 7\",\n",
        "        \"- Context size: 384-512 tokens\",\n",
        "        \"\",\n",
        "        \"## Analyzed Sentences\",\n",
        "        \"\"\n",
        "    ]\n",
        "\n",
        "    # Add analyzed sentences\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        report.append(f\"{i+1}. `{sentence[:100]}{'...' if len(sentence) > 100 else ''}`\")\n",
        "\n",
        "    report.extend([\n",
        "        \"\",\n",
        "        \"## Attention Head Behavior Analysis\",\n",
        "        \"\",\n",
        "        \"Attention heads exhibit different behaviors across sentences and language contexts. We categorize heads into several behavioral types:\",\n",
        "        \"\",\n",
        "        \"- **Diagonal/Self**: Heads that primarily attend to the current token or immediately adjacent tokens\",\n",
        "        \"- **Local Context**: Heads that focus on nearby tokens within a short window\",\n",
        "        \"- **Global/Broad**: Heads that distribute attention broadly across the sequence\",\n",
        "        \"- **Token-Specific**: Heads that focus attention on specific token types (e.g., punctuation, certain parts of speech)\",\n",
        "        \"\"\n",
        "    ])\n",
        "\n",
        "    # Add behavior summary image\n",
        "    behavior_summary_path = os.path.join(viz_dir, \"summaries\", \"head_behavior_summary.png\")\n",
        "    if os.path.exists(behavior_summary_path):\n",
        "        report.append(f\"![Head Behavior Summary](summaries/head_behavior_summary.png)\")\n",
        "        report.append(\"\")\n",
        "\n",
        "    # Add behavior table\n",
        "    behavior_table_path = os.path.join(viz_dir, \"summaries\", \"head_behavior_table.csv\")\n",
        "    if os.path.exists(behavior_table_path):\n",
        "        try:\n",
        "            df = pd.read_csv(behavior_table_path)\n",
        "            markdown_table = []\n",
        "\n",
        "            # Create header\n",
        "            header = \"| \" + \" | \".join(df.columns) + \" |\"\n",
        "            separator = \"| \" + \" | \".join([\"---\"] * len(df.columns)) + \" |\"\n",
        "            markdown_table.append(header)\n",
        "            markdown_table.append(separator)\n",
        "\n",
        "            # Add rows\n",
        "            for _, row in df.iterrows():\n",
        "                row_str = \"| \" + \" | \".join([str(val) for val in row.values]) + \" |\"\n",
        "                markdown_table.append(row_str)\n",
        "\n",
        "            report.append(\"### Head Behavior Summary Table\")\n",
        "            report.append(\"\")\n",
        "            report.append(\"\\n\".join(markdown_table))\n",
        "            report.append(\"\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding behavior table: {e}\")\n",
        "\n",
        "    # Add clustering analysis\n",
        "    report.extend([\n",
        "        \"## Head Similarity Analysis\",\n",
        "        \"\",\n",
        "        \"Heads that share similar attention patterns across sentences may be performing related functions. We analyze the similarity between heads:\",\n",
        "        \"\"\n",
        "    ])\n",
        "\n",
        "    # Add similarity matrix image\n",
        "    similarity_path = os.path.join(viz_dir, \"summaries\", \"head_similarity_matrix.png\")\n",
        "    if os.path.exists(similarity_path):\n",
        "        report.append(f\"![Head Similarity Matrix](summaries/head_similarity_matrix.png)\")\n",
        "        report.append(\"\")\n",
        "\n",
        "    # Add clustering image\n",
        "    clustering_path = os.path.join(viz_dir, \"summaries\", \"head_clustering_dendrogram.png\")\n",
        "    if os.path.exists(clustering_path):\n",
        "        report.append(f\"![Head Clustering](summaries/head_clustering_dendrogram.png)\")\n",
        "        report.append(\"\")\n",
        "\n",
        "    # Add interesting patterns section\n",
        "    report.extend([\n",
        "        \"## Interesting Attention Patterns\",\n",
        "        \"\",\n",
        "        \"Here we highlight some of the most interesting attention patterns found:\",\n",
        "        \"\"\n",
        "    ])\n",
        "\n",
        "    # Collect interesting token-specific patterns\n",
        "    interesting_patterns = []\n",
        "    for sentence_idx, (sentence, pattern_data, _) in enumerate(all_sentences_data):\n",
        "        if \"token_specific\" not in pattern_data:\n",
        "            continue\n",
        "\n",
        "        for data in pattern_data[\"token_specific\"]:\n",
        "            if len(data) < 4:\n",
        "                continue\n",
        "\n",
        "            layer_idx, head_idx = data[0], data[1]\n",
        "            top_attended = data[3]\n",
        "            if top_attended:\n",
        "                for from_idx, to_idx, from_token, to_token, attn_weight in sorted(top_attended, key=lambda x: x[4], reverse=True)[:2]:\n",
        "                    interesting_patterns.append({\n",
        "                        \"sentence_idx\": sentence_idx,\n",
        "                        \"sentence\": sentence,\n",
        "                        \"layer_idx\": layer_idx,\n",
        "                        \"head_idx\": head_idx,\n",
        "                        \"from_token\": from_token,\n",
        "                        \"to_token\": to_token,\n",
        "                        \"attn_weight\": attn_weight\n",
        "                    })\n",
        "\n",
        "    # Sort by attention weight\n",
        "    interesting_patterns = sorted(interesting_patterns, key=lambda x: x[\"attn_weight\"], reverse=True)\n",
        "\n",
        "    # Add top patterns to report\n",
        "    for i, pattern in enumerate(interesting_patterns[:5]):\n",
        "        report.append(f\"### Pattern {i+1}: Layer {pattern['layer_idx']+1}, Head {pattern['head_idx']+1}\")\n",
        "        report.append(\"\")\n",
        "        report.append(f\"- **From Token**: `{pattern['from_token']}`\")\n",
        "        report.append(f\"- **To Token**: `{pattern['to_token']}`\")\n",
        "        report.append(f\"- **Attention Weight**: {pattern['attn_weight']:.3f}\")\n",
        "        report.append(f\"- **Sentence Context**: `{pattern['sentence'][:100]}{'...' if len(pattern['sentence']) > 100 else ''}`\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Add heatmap image if available\n",
        "        heatmap_path = os.path.join(\n",
        "            viz_dir,\n",
        "            \"heatmaps\",\n",
        "            f\"heatmap_L{pattern['layer_idx']+1}_H{pattern['head_idx']+1}_sentence_{pattern['sentence_idx']+1}.png\"\n",
        "        )\n",
        "        if os.path.exists(heatmap_path):\n",
        "            report.append(f\"![Attention Heatmap](heatmaps/heatmap_L{pattern['layer_idx']+1}_H{pattern['head_idx']+1}_sentence_{pattern['sentence_idx']+1}.png)\")\n",
        "            report.append(\"\")\n",
        "\n",
        "    # Add visualization examples\n",
        "    report.extend([\n",
        "        \"## Visualization Examples\",\n",
        "        \"\",\n",
        "        \"This section shows examples of different visualizations created for the attention patterns.\",\n",
        "        \"\"\n",
        "    ])\n",
        "\n",
        "    # Add heatmap examples\n",
        "    report.append(\"### Attention Heatmaps\")\n",
        "    report.append(\"\")\n",
        "    report.append(\"Heatmaps show the attention weight from each token (y-axis) to each token (x-axis):\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    # Find a few good heatmap examples\n",
        "    heatmap_dir = os.path.join(viz_dir, \"heatmaps\")\n",
        "    if os.path.exists(heatmap_dir):\n",
        "        heatmap_files = os.listdir(heatmap_dir)\n",
        "        if heatmap_files:\n",
        "            for file in heatmap_files[:3]:  # Show up to 3 examples\n",
        "                report.append(f\"![Attention Heatmap](heatmaps/{file})\")\n",
        "                report.append(\"\")\n",
        "\n",
        "    # Add graph examples\n",
        "    report.append(\"### Attention Flow Graphs\")\n",
        "    report.append(\"\")\n",
        "    report.append(\"Flow graphs visualize how attention flows between tokens, with edge thickness indicating attention strength:\")\n",
        "    report.append(\"\")\n",
        "\n",
        "    # Find graph examples\n",
        "    graph_dir = os.path.join(viz_dir, \"graphs\")\n",
        "    if os.path.exists(graph_dir):\n",
        "        graph_files = os.listdir(graph_dir)\n",
        "        if graph_files:\n",
        "            for file in graph_files[:3]:  # Show up to 3 examples\n",
        "                report.append(f\"![Attention Flow](graphs/{file})\")\n",
        "                report.append(\"\")\n",
        "\n",
        "    # Write report to file\n",
        "    report_path = os.path.join(viz_dir, \"interpretability_report.md\")\n",
        "    with open(report_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(report))\n",
        "\n",
        "    print(f\"Created interpretability report at {report_path}\")\n",
        "    return report_path\n",
        "\n",
        "def run_comprehensive_attention_analysis(model, encoder, test_sentences, save_dir, model_name=\"transformer\"):\n",
        "    \"\"\"\n",
        "    Run a comprehensive attention analysis on a model\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"RUNNING COMPREHENSIVE ATTENTION ANALYSIS: {model_name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Create visualization directory\n",
        "    viz_dir = create_visualization_dir(save_dir, model_name)\n",
        "\n",
        "    # Store results for all sentences\n",
        "    all_sentences_data = []\n",
        "    all_sentences_metrics = []\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    for sentence_idx, sentence in enumerate(test_sentences):\n",
        "        sentence_name = f\"sentence_{sentence_idx+1}\"\n",
        "        print(f\"\\nAnalyzing sentence {sentence_idx+1}/{len(test_sentences)}: {sentence[:50]}...\")\n",
        "\n",
        "        # Tokenize sentence\n",
        "        token_ids = encoder.encode(sentence)\n",
        "        tokens_str = [clean_token_for_display(encoder.decode([tok_id]), encoder) for tok_id in token_ids]\n",
        "\n",
        "        # Truncate if needed\n",
        "        max_tokens = min(512, model.blocks[0].attn.rope.max_seq_len_cached)\n",
        "        if len(token_ids) > max_tokens:\n",
        "            print(f\"  Truncating from {len(token_ids)} to {max_tokens} tokens\")\n",
        "            token_ids = token_ids[:max_tokens]\n",
        "            tokens_str = tokens_str[:max_tokens]\n",
        "\n",
        "        # Prepare input\n",
        "        input_tensor = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
        "\n",
        "        # Get attention weights\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                _, all_layer_attention_weights = model(input_tensor, return_attention_weights=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error getting attention weights: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Extract patterns\n",
        "        pattern_data, head_metrics = extract_attention_patterns(tokens_str, all_layer_attention_weights)\n",
        "\n",
        "        # Store for overall analysis\n",
        "        all_sentences_data.append((sentence, pattern_data, head_metrics))\n",
        "        all_sentences_metrics.append(head_metrics)\n",
        "\n",
        "        # Visualize patterns for this sentence\n",
        "        n_layers = len(all_layer_attention_weights)\n",
        "        n_heads = all_layer_attention_weights[0].size(1)\n",
        "\n",
        "        # Create visualizations (only for a subset of heads to keep it manageable)\n",
        "        interesting_layer_head_pairs = []\n",
        "\n",
        "        # Add diagonal pattern heads\n",
        "        for layer_idx, head_idx, _, _ in pattern_data[\"diagonal\"][:2]:\n",
        "            interesting_layer_head_pairs.append((layer_idx, head_idx, \"diagonal\"))\n",
        "\n",
        "        # Add local pattern heads\n",
        "        for layer_idx, head_idx, _ in pattern_data[\"local\"][:2]:\n",
        "            interesting_layer_head_pairs.append((layer_idx, head_idx, \"local\"))\n",
        "\n",
        "        # Add token-specific pattern heads\n",
        "        for layer_idx, head_idx, _, _ in pattern_data[\"token_specific\"][:2]:\n",
        "            interesting_layer_head_pairs.append((layer_idx, head_idx, \"token_specific\"))\n",
        "\n",
        "        # Add global pattern heads\n",
        "        for layer_idx, head_idx, _ in pattern_data[\"global\"][:2]:\n",
        "            interesting_layer_head_pairs.append((layer_idx, head_idx, \"global\"))\n",
        "\n",
        "        # Ensure we have at least some pairs to visualize\n",
        "        if not interesting_layer_head_pairs:\n",
        "            # Just pick some heads\n",
        "            interesting_layer_head_pairs = [\n",
        "                (0, 0, \"first\"),  # First layer, first head\n",
        "                (n_layers-1, 0, \"last\"),  # Last layer, first head\n",
        "                (n_layers//2, n_heads//2, \"middle\")  # Middle layer, middle head\n",
        "            ]\n",
        "\n",
        "        # Create visualizations for interesting heads\n",
        "        print(f\"  Creating visualizations for {len(interesting_layer_head_pairs)} interesting heads...\")\n",
        "        for layer_idx, head_idx, pattern_type in interesting_layer_head_pairs:\n",
        "            if layer_idx < 0 or layer_idx >= n_layers or head_idx < 0 or head_idx >= n_heads:\n",
        "                continue\n",
        "\n",
        "            layer_weights = all_layer_attention_weights[layer_idx]\n",
        "            head_weights = layer_weights[0, head_idx]\n",
        "\n",
        "            # Heatmap\n",
        "            plot_attention_heatmap(\n",
        "                tokens_str,\n",
        "                head_weights,\n",
        "                layer_idx,\n",
        "                head_idx,\n",
        "                viz_dir,\n",
        "                sentence_name\n",
        "            )\n",
        "\n",
        "            # Flow graph\n",
        "            plot_attention_flow_graph(\n",
        "                tokens_str,\n",
        "                head_weights,\n",
        "                layer_idx,\n",
        "                head_idx,\n",
        "                viz_dir,\n",
        "                sentence_name\n",
        "            )\n",
        "\n",
        "    # Create cross-sentence analysis visualizations\n",
        "    print(\"\\nCreating cross-sentence analysis visualizations...\")\n",
        "\n",
        "    if not all_sentences_data:\n",
        "        print(\"No attention data collected. Cannot create analysis visualizations.\")\n",
        "        return {\"viz_dir\": viz_dir, \"error\": \"No attention data collected\"}\n",
        "\n",
        "    # Visualize attention patterns\n",
        "    visualize_attention_patterns(\n",
        "        all_sentences_data[0][1],  # Use patterns from first sentence for simplicity\n",
        "        n_layers,\n",
        "        n_heads,\n",
        "        viz_dir\n",
        "    )\n",
        "\n",
        "    # Create head similarity matrix\n",
        "    similarity_matrix, all_heads = create_head_similarity_matrix(all_sentences_metrics)\n",
        "\n",
        "    # Plot similarity matrix\n",
        "    plot_head_similarity_matrix(similarity_matrix, all_heads, viz_dir)\n",
        "\n",
        "    # Cluster attention heads\n",
        "    cluster_attention_heads(similarity_matrix, all_heads, viz_dir)\n",
        "\n",
        "    # Create behavior summary\n",
        "    create_attention_behavior_summary(all_sentences_data, viz_dir)\n",
        "\n",
        "    # Generate comprehensive report\n",
        "    report_path = create_interpretability_report(all_sentences_data, model_name, viz_dir, encoder)\n",
        "\n",
        "    print(\"\\nComprehensive attention analysis complete!\")\n",
        "    print(f\"Report: {report_path}\")\n",
        "    print(f\"Visualizations: {viz_dir}\")\n",
        "\n",
        "    # In Colab, download the results immediately\n",
        "    if IN_COLAB:\n",
        "        auto_download_results(viz_dir, f\"pico_llm_visualizations_{model_name}\")\n",
        "\n",
        "    return {\n",
        "        \"viz_dir\": viz_dir,\n",
        "        \"report_path\": report_path,\n",
        "        \"all_sentences_data\": all_sentences_data,\n",
        "    }\n",
        "\n",
        "#################################################################################\n",
        "# PART 9: Main Function\n",
        "#################################################################################\n",
        "\n",
        "def main():\n",
        "    global tracker, global_encoder\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"   PICO-LLM: ENHANCED INTERPRETABILITY ANALYSIS   \")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Colab-specific warning\n",
        "    if IN_COLAB:\n",
        "        print(\"\\nIMPORTANT: Running in Google Colab environment.\")\n",
        "        print(\"Files will be automatically downloaded to your local computer\")\n",
        "        print(\"at key points to prevent data loss when Colab disconnects.\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        global_encoder = tiktoken.get_encoding(\"gpt2\")\n",
        "        print(f\"Global tokenizer: GPT-2 (Vocab: {global_encoder.n_vocab}, EOT: {global_encoder.eot_token})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal: Tokenizer init failed: {e}\")\n",
        "        return\n",
        "\n",
        "    tracker_root_dir = \"pico_llm_interpretability_experiments\"\n",
        "    tracker = ExperimentTracker(root_dir=tracker_root_dir)\n",
        "\n",
        "    # Enhanced configuration\n",
        "    cfg = ExperimentConfig(\n",
        "        # Model architecture\n",
        "        embed_size=448,\n",
        "        transformer_n_blocks=8,\n",
        "        transformer_n_heads=7,  # Keep head_size divisible (448/7=64)\n",
        "        block_size=512,  # Slightly larger context window\n",
        "        dropout_rate=0.1,  # Standard dropout for base model\n",
        "\n",
        "        # Training parameters - Using smaller dataset split for faster execution\n",
        "        base_dataset_name=\"wikitext\",\n",
        "        base_dataset_config=\"wikitext-103-v1\",\n",
        "        base_dataset_split=\"train[:20%]\",  # Use 20% to save time\n",
        "        base_num_epochs=10,  # Fewer epochs for quick execution\n",
        "        base_learning_rate=1e-4,\n",
        "        weight_decay=0.01,\n",
        "        warmup_ratio=0.06,\n",
        "\n",
        "        # Save path\n",
        "        base_model_save_path=\"results/base_transformer_for_interpretability.pt\",\n",
        "\n",
        "        # Better interpretability sample sentences\n",
        "        interpret_sample_sentences=get_test_sentences(max_per_category=1)\n",
        "    )\n",
        "\n",
        "    print(f\"Device: {cfg.device_id}\")\n",
        "    print(\"\\nPipeline Options:\")\n",
        "    print(\"  1. Pre-train Base Transformer Model\")\n",
        "    print(\"  2. Run Enhanced Interpretability Analysis\")\n",
        "    print(\"  3. Run BOTH: Pre-train then Analyze\")\n",
        "    choice = input(\"Enter your choice (1-3): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        print(\"\\n--- ACTION: Pre-training Base Model ---\")\n",
        "        train_base_transformer(cfg, tracker, global_encoder)\n",
        "\n",
        "        # In Colab, make sure to download the model\n",
        "        if IN_COLAB:\n",
        "            print(\"\\nTraining complete! Downloading model and results...\")\n",
        "            auto_download_results(\"results\", \"pico_llm_trained_model\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        print(\"\\n--- ACTION: Running Enhanced Interpretability Analysis ---\")\n",
        "        model_path = input(f\"Path to pre-trained model for interpretation (default: '{cfg.interpret_model_path}'): \").strip()\n",
        "        if model_path:\n",
        "            cfg.interpret_model_path = model_path\n",
        "\n",
        "        if not os.path.exists(cfg.interpret_model_path):\n",
        "            print(f\"Error: Model for interpretation not found at '{cfg.interpret_model_path}'. Please train or provide a valid path.\")\n",
        "            return\n",
        "\n",
        "        # Ask which test sentence categories to use\n",
        "        print(\"\\nAvailable test sentence categories:\")\n",
        "        for i, category in enumerate(ALL_TEST_CATEGORIES.keys()):\n",
        "            print(f\"  {i+1}. {category}\")\n",
        "\n",
        "        category_input = input(\"Enter category numbers to use (comma-separated, empty for all): \").strip()\n",
        "        if category_input:\n",
        "            try:\n",
        "                selected_indices = [int(idx.strip()) - 1 for idx in category_input.split(\",\")]\n",
        "                selected_categories = [list(ALL_TEST_CATEGORIES.keys())[idx] for idx in selected_indices if 0 <= idx < len(ALL_TEST_CATEGORIES)]\n",
        "                cfg.interpret_sample_sentences = get_test_sentences(categories=selected_categories, max_per_category=1)\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Using default test sentences.\")\n",
        "\n",
        "        # Load model\n",
        "        device = torch.device(cfg.device_id)\n",
        "        model = TransformerModel(\n",
        "            global_encoder.n_vocab,\n",
        "            cfg.embed_size,\n",
        "            cfg.transformer_n_heads,\n",
        "            cfg.transformer_n_blocks,\n",
        "            cfg.block_size,\n",
        "            cfg.dropout_rate\n",
        "        ).to(device)\n",
        "\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(cfg.interpret_model_path, map_location=device))\n",
        "            print(f\"Model loaded successfully from {cfg.interpret_model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return\n",
        "\n",
        "        # Run comprehensive analysis\n",
        "        results = run_comprehensive_attention_analysis(\n",
        "            model,\n",
        "            global_encoder,\n",
        "            cfg.interpret_sample_sentences,\n",
        "            tracker.root_dir,\n",
        "            model_name=os.path.basename(cfg.interpret_model_path).replace('.pt', '')\n",
        "        )\n",
        "\n",
        "        # In Colab, force download of visualizations\n",
        "        if IN_COLAB:\n",
        "            print(\"\\nAnalysis complete! Downloading visualizations...\")\n",
        "            auto_download_results(tracker.root_dir, \"pico_llm_attention_analysis\")\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        print(\"\\n--- ACTION: Pre-training Base Model and Running Analysis ---\")\n",
        "\n",
        "        # First train the model\n",
        "        print(\"\\nStep 1: Pre-training Base Model...\")\n",
        "        model = train_base_transformer(cfg, tracker, global_encoder)\n",
        "\n",
        "        if model is None:\n",
        "            print(\"Pre-training failed. Cannot proceed to analysis.\")\n",
        "            return\n",
        "\n",
        "        # Ask which test sentence categories to use\n",
        "        print(\"\\nAvailable test sentence categories:\")\n",
        "        for i, category in enumerate(ALL_TEST_CATEGORIES.keys()):\n",
        "            print(f\"  {i+1}. {category}\")\n",
        "\n",
        "        category_input = input(\"Enter category numbers to use (comma-separated, empty for all): \").strip()\n",
        "        if category_input:\n",
        "            try:\n",
        "                selected_indices = [int(idx.strip()) - 1 for idx in category_input.split(\",\")]\n",
        "                selected_categories = [list(ALL_TEST_CATEGORIES.keys())[idx] for idx in selected_indices if 0 <= idx < len(ALL_TEST_CATEGORIES)]\n",
        "                cfg.interpret_sample_sentences = get_test_sentences(categories=selected_categories, max_per_category=1)\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Using default test sentences.\")\n",
        "\n",
        "        # Then run analysis\n",
        "        print(\"\\nStep 2: Running Enhanced Interpretability Analysis...\")\n",
        "        results = run_comprehensive_attention_analysis(\n",
        "            model,\n",
        "            global_encoder,\n",
        "            cfg.interpret_sample_sentences,\n",
        "            tracker.root_dir,\n",
        "            model_name=os.path.basename(cfg.base_model_save_path).replace('.pt', '')\n",
        "        )\n",
        "\n",
        "        # In Colab, force download of complete results\n",
        "        if IN_COLAB:\n",
        "            print(\"\\nAnalysis complete! Downloading all results...\")\n",
        "            auto_download_results(tracker.root_dir, \"pico_llm_complete_results\")\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid choice.\")\n",
        "\n",
        "    if tracker.experiments:\n",
        "        print(\"\\nGenerating final reports (if any training was done)...\")\n",
        "        tracker.plot_training_history()\n",
        "        tracker.save_comparison_report()\n",
        "    else:\n",
        "        print(\"No experiments run/tracked. Skipping reports.\")\n",
        "\n",
        "    # Final download prompt for Colab\n",
        "    if IN_COLAB:\n",
        "        print(\"\\nIMPORTANT: Make sure all downloads have completed successfully!\")\n",
        "        print(\"If you missed any downloads, use the following code to download results:\")\n",
        "        print(\"from google.colab import files\")\n",
        "        print(\"!zip -r all_results.zip results/ pico_llm_interpretability_experiments/\")\n",
        "        print(\"files.download('all_results.zip')\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PICO-LLM EXECUTION COMPLETED.\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 10: TENSORRT OPTIMIZATION & PERFORMANCE ACCELERATION\n",
        "# ============================================================================\n",
        "\n",
        "# Install TensorRT and Torch-TensorRT for inference optimization\n",
        "%pip install --upgrade nvidia-tensorrt torch-tensorrt -f https://storage.googleapis.com/torch-tensorrt-nightly-wheels/torch2.3-cu121-20240409.html\n",
        "\n",
        "# Import additional dependencies for TensorRT optimization\n",
        "import torch_tensorrt\n",
        "import time\n",
        "\n",
        "def load_model_for_optimization():\n",
        "    \"\"\"Load the trained transformer model for TensorRT optimization\"\"\"\n",
        "    print(\"Loading configuration and model for TensorRT optimization...\")\n",
        "    cfg = ExperimentConfig()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Initialize model structure\n",
        "    print(\"Initializing PyTorch model structure...\")\n",
        "    pytorch_model = TransformerModel(\n",
        "        global_encoder.n_vocab,\n",
        "        cfg.embed_size,\n",
        "        cfg.transformer_n_heads,\n",
        "        cfg.transformer_n_blocks,\n",
        "        cfg.block_size,\n",
        "        cfg.dropout_rate\n",
        "    ).to(device).eval()\n",
        "    \n",
        "    # Load trained weights\n",
        "    model_path = cfg.base_model_save_path\n",
        "    pytorch_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(f\"✅ Successfully loaded PyTorch model from {model_path}\")\n",
        "    \n",
        "    return pytorch_model, cfg, device\n",
        "\n",
        "def convert_to_tensorrt(pytorch_model, device, sequence_length=128):\n",
        "    \"\"\"Convert PyTorch model to TensorRT optimized version with FP16 precision\"\"\"\n",
        "    print(\"\\nCompiling model with TensorRT (FP16)... This may take a few minutes.\")\n",
        "    \n",
        "    # Create representative input for compilation\n",
        "    dummy_input = torch.randint(0, global_encoder.n_vocab, (1, sequence_length), dtype=torch.long).to(device)\n",
        "    \n",
        "    # Compile with TensorRT using FP16 precision for speed and memory efficiency\n",
        "    trt_model_fp16 = torch_tensorrt.compile(\n",
        "        pytorch_model,\n",
        "        inputs=[dummy_input],\n",
        "        enabled_precisions={torch.half}\n",
        "    )\n",
        "    \n",
        "    # Save the optimized engine\n",
        "    trt_model_path = \"results/pico_llm_trt_fp16.ts\"\n",
        "    torch.jit.save(trt_model_fp16, trt_model_path)\n",
        "    print(f\"✅ TensorRT compilation complete! Engine saved to {trt_model_path}\")\n",
        "    \n",
        "    return trt_model_fp16, trt_model_path\n",
        "\n",
        "def benchmark_model_performance(model_name, model, input_tensor, num_runs=200, warmup_runs=50):\n",
        "    \"\"\"Comprehensive benchmarking function for model inference latency\"\"\"\n",
        "    print(f\"\\n--- Benchmarking {model_name} ---\")\n",
        "    latencies = []\n",
        "    \n",
        "    # Warmup phase to stabilize GPU clocks and cache\n",
        "    print(f\"Running {warmup_runs} warmup runs...\")\n",
        "    for _ in range(warmup_runs):\n",
        "        _ = model(input_tensor)\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    # Actual timing runs\n",
        "    print(f\"Running {num_runs} timed runs...\")\n",
        "    for _ in range(num_runs):\n",
        "        start_time = time.perf_counter()\n",
        "        _ = model(input_tensor)\n",
        "        torch.cuda.synchronize()  # Ensure complete GPU execution\n",
        "        end_time = time.perf_counter()\n",
        "        latencies.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
        "        \n",
        "    avg_latency = sum(latencies) / len(latencies)\n",
        "    min_latency = min(latencies)\n",
        "    max_latency = max(latencies)\n",
        "    \n",
        "    print(f\"Average latency over {num_runs} runs: {avg_latency:.4f} ms\")\n",
        "    print(f\"Min latency: {min_latency:.4f} ms, Max latency: {max_latency:.4f} ms\")\n",
        "    \n",
        "    return avg_latency\n",
        "\n",
        "def run_performance_comparison():\n",
        "    \"\"\"Run comprehensive performance comparison between PyTorch and TensorRT\"\"\"\n",
        "    # Load model\n",
        "    pytorch_model, cfg, device = load_model_for_optimization()\n",
        "    \n",
        "    # Convert to TensorRT\n",
        "    trt_model_fp16, trt_model_path = convert_to_tensorrt(pytorch_model, device)\n",
        "    \n",
        "    # Prepare benchmark input\n",
        "    input_for_benchmark = torch.randint(0, global_encoder.n_vocab, (1, 128), dtype=torch.long).to(device)\n",
        "    \n",
        "    # Run benchmarks\n",
        "    pytorch_latency = benchmark_model_performance(\"Original PyTorch Model\", pytorch_model, input_for_benchmark)\n",
        "    trt_latency = benchmark_model_performance(\"TensorRT FP16 Model\", trt_model_fp16, input_for_benchmark)\n",
        "    \n",
        "    # Calculate and display results\n",
        "    speedup = pytorch_latency / trt_latency\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🚀 TENSORRT OPTIMIZATION RESULTS 🚀\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"TensorRT Speedup: {speedup:.2f}x\")\n",
        "    print(f\"Latency reduced from {pytorch_latency:.2f}ms to {trt_latency:.2f}ms\")\n",
        "    print(f\"Performance improvement: {((speedup - 1) * 100):.1f}%\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Memory usage comparison\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"\\n--- MEMORY USAGE ANALYSIS ---\")\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        _ = pytorch_model(input_for_benchmark)\n",
        "        pytorch_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
        "        \n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        _ = trt_model_fp16(input_for_benchmark)\n",
        "        trt_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
        "        \n",
        "        print(f\"PyTorch peak memory: {pytorch_memory:.1f} MB\")\n",
        "        print(f\"TensorRT peak memory: {trt_memory:.1f} MB\")\n",
        "        memory_savings = ((pytorch_memory - trt_memory) / pytorch_memory) * 100\n",
        "        print(f\"Memory savings: {memory_savings:.1f}%\")\n",
        "    \n",
        "    return pytorch_model, trt_model_fp16\n",
        "\n",
        "def advanced_tensorrt_optimization_demo():\n",
        "    \"\"\"Demonstrate advanced TensorRT optimization techniques\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ADVANCED TENSORRT OPTIMIZATION TECHNIQUES\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Load and optimize models\n",
        "    pytorch_model, trt_model_fp16 = run_performance_comparison()\n",
        "    \n",
        "    # Save optimized model\n",
        "    trt_save_path = \"results/pico_llm_tensorrt_optimized.pt\"\n",
        "    torch.jit.save(trt_model_fp16, trt_save_path)\n",
        "    print(f\"\\n✅ TensorRT model saved to: {trt_save_path}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TENSORRT OPTIMIZATION COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Your transformer model has been successfully optimized with TensorRT.\")\n",
        "    print(\"Use the TensorRT model for production inference to achieve maximum performance.\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return trt_model_fp16\n",
        "\n",
        "# Run the complete TensorRT optimization pipeline\n",
        "# Uncomment the line below to execute the optimization\n",
        "# optimized_model = advanced_tensorrt_optimization_demo()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMoTyywPx9JSOFISJe2BIIT",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
